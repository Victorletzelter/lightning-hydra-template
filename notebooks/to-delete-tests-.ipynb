{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Example usage\n",
    "N, T, B = 2, 5, 16\n",
    "in_features, out_features = B//4, 10\n",
    "\n",
    "# create input tensor of shape [NxTxB/4]\n",
    "x = torch.randn(N, T, in_features)\n",
    "\n",
    "x = x.view(-1, x.shape[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19699/246092982.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubset_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_subsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mframe_recall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_frame_recall'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubset_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdoa_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_doa_error'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubset_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_19699/246092982.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubset_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_subsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mframe_recall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_frame_recall'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubset_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdoa_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_doa_error'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubset_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "outputs = [{'test_frame_recall': torch.tensor(0.3688), 'test_doa_error': torch.tensor(float('nan'))}]\n",
    "num_subsets = len(outputs)\n",
    "\n",
    "for subset_idx in range(num_subsets):\n",
    "    frame_recall = torch.stack([x['test_frame_recall'] for x in outputs[subset_idx]])\n",
    "    doa_error = torch.stack([x['test_doa_error'] for x in outputs[subset_idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3688])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([x['test_frame_recall'] for x in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# apply linear layer to each time step\n",
    "x = torch.nn.Linear(in_features, out_features)(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reshape output tensor to [N x T x S]\n",
    "x = x.view(*x.shape[:-1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PackedSequence' object has no attribute 'view'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_43939/1109528434.py\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mpacked_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mlinear_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# output tensor of shape [N x T x S]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_43939/1109528434.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# reshape input tensor to [N*T x B/4]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# apply linear layer to each time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PackedSequence' object has no attribute 'view'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # reshape input tensor to [N*T x B/4]\n",
    "        x = x.view(-1, x.shape[-1])\n",
    "\n",
    "        # apply linear layer to each time step\n",
    "        x = self.linear(x)\n",
    "\n",
    "        # reshape output tensor to [N x T x S]\n",
    "        x = x.view(*x.shape[:-1], -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Example usage\n",
    "N, T, B = 2, 5, 16\n",
    "in_features, out_features = B//4, 10\n",
    "\n",
    "# create input tensor of shape [NxTxB/4]\n",
    "x = torch.randn(N, T, in_features)\n",
    "\n",
    "# pack input tensor and apply time-distributed linear layer\n",
    "packed_x = rnn_utils.pack_padded_sequence(x, lengths=[T]*N, batch_first=True, enforce_sorted=False)\n",
    "linear_layer = TimeDistributed(in_features, out_features)\n",
    "y, _ = rnn_utils.pad_packed_sequence(linear_layer(packed_x)[0], batch_first=True)\n",
    "\n",
    "# output tensor of shape [N x T x S]\n",
    "print(y.shape)  # should be [2, 5, 10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "class TimeDistributedFC(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for applying time distributed fully connected layers with input tensors \n",
    "    of shape  [N, T, in_features] where N is the batch_size, T is the number of time step,\n",
    "    and in_features is the number of input features at each time step. \n",
    "    Output tensors will have shape [N, T, out_features], where out_features is the number \n",
    "    of output features at each time step.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(TimeDistributedFC, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        N,T = x.shape[0],x.shape[1]\n",
    "        \n",
    "        # reshape input tensor to [N*T x B/4]\n",
    "        x = x.view(-1, x.shape[-1])\n",
    "\n",
    "        # # apply linear layer to each time step\n",
    "        # x = self.linear(x)\n",
    "\n",
    "        # reshape output tensor to [N x T x S]\n",
    "        x = x.view(N, T,-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "N, T, B = 2, 5, 16\n",
    "in_features, out_features = B//4, 10\n",
    "\n",
    "# create input tensor of shape [NxTxB/4]\n",
    "x = torch.randn(N, T, in_features)\n",
    "\n",
    "# # pack input tensor and apply time-distributed linear layer\n",
    "# packed_x = rnn_utils.pack_padded_sequence(x, lengths=[T]*N, batch_first=True, enforce_sorted=False)\n",
    "linear_layer = TimeDistributedFC(in_features, out_features)\n",
    "# y, _ = rnn_utils.pad_packed_sequence(linear_layer(packed_x)[0], batch_first=True)\n",
    "y = linear_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3244,  0.4101, -1.5298,  0.9352])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[i,j,k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N = 2\n",
    "T = 20\n",
    "directions_of_arrival = []\n",
    "num_hypothesis = 10\n",
    "        \n",
    "for i in range(num_hypothesis) :\n",
    "    directions_of_arrival.append(torch.randn(size = (N,T,2))) # Size [NxTx2]\n",
    "    \n",
    "hyp_stacked = torch.stack(directions_of_arrival, dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "T = 20\n",
    "directions_of_arrival = []\n",
    "num_hypothesis = 10\n",
    "\n",
    "directions_of_arrival = torch.randn(size = (N,T,2*num_hypothesis)) # Size [NxTx(2*num_hypothesis)]\n",
    "hyps_splitted = torch.split(directions_of_arrival, [2 for i in range(num_hypothesis)], 2) #num_hypothesis-uples of elements of shape [N,T,2]\n",
    "hyps_stacked = torch.stack([h for h in hyps_splitted], dim=2) #Tuples of elements of shape [N,T,num_hypothesis,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 20, 10, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hyps_splitted)\n",
    "hyps_splitted[0].shape\n",
    "hyps_stacked.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "source_activity_target_t = torch.tensor([[1, 0, 1], [0, 1, 0]])\n",
    "direction_of_arrival_target_t = torch.tensor(\n",
    "    [[[1, 2], [3, 4], [5, 6]], [[7, 8], [9, 10], [11, 12]]]\n",
    ")\n",
    "\n",
    "direction_of_arrival_target_t[source_activity_target_t == 0, :] = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[   1,    2],\n",
       "         [1000, 1000],\n",
       "         [   5,    6]],\n",
       "\n",
       "        [[1000, 1000],\n",
       "         [   9,   10],\n",
       "         [1000, 1000]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "direction_of_arrival_target_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [{'test_frame_recall': torch.tensor(0.9925), 'test_doa_error': torch.tensor(16.2175)}, \n",
    " {'test_frame_recall': torch.tensor(0.9950), 'test_doa_error': torch.tensor(10.0907)}, \n",
    " {'test_frame_recall': torch.tensor(0.9950), 'test_doa_error': torch.tensor(14.7058)}, \n",
    " {'test_frame_recall': torch.tensor(0.9862), 'test_doa_error': torch.tensor(8.9629)}, \n",
    " {'test_frame_recall': torch.tensor(0.9613), 'test_doa_error': torch.tensor(16.7399)}, \n",
    " {'test_frame_recall': torch.tensor(0.9975), 'test_doa_error': torch.tensor(16.7870)}, \n",
    " {'test_frame_recall': torch.tensor(0.9775), 'test_doa_error': torch.tensor(10.8708)}, \n",
    " {'test_frame_recall': torch.tensor(0.9962), 'test_doa_error': torch.tensor(12.1684)}, \n",
    " {'test_frame_recall': torch.tensor(0.9912), 'test_doa_error': torch.tensor(14.0261)}, \n",
    " {'test_frame_recall': torch.tensor(0.9900), 'test_doa_error': torch.tensor(12.9945)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "            'model': [], 'dataset': [], 'fold_idx': [], 'subset_idx': [], 'frame_recall': [], 'doa_error': []\n",
    "        }\n",
    "frame_recall = torch.stack([x['test_frame_recall'] for x in outputs]).detach().cpu().numpy()\n",
    "doa_error = torch.stack([x['test_doa_error'] for x in outputs]).detach().cpu().numpy()\n",
    "\n",
    "num_sequences = len(frame_recall)\n",
    "\n",
    "for seq_idx in range(num_sequences):\n",
    "    results['model'].append('name')\n",
    "    results['dataset'].append('data')\n",
    "    results['fold_idx'].append(0)\n",
    "    results['subset_idx'].append(0)\n",
    "    results['frame_recall'].append(frame_recall[seq_idx])\n",
    "    results['doa_error'].append(doa_error[seq_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9925, 0.995, 0.995, 0.9862, 0.9613, 0.9975, 0.9775, 0.9962, 0.9912, 0.99]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['frame_recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "from importlib.util import find_spec\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List\n",
    "from pytorch_lightning import Callback\n",
    "from pytorch_lightning.loggers import Logger\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "from itertools import permutations\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.modules.loss import _Loss\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spherical_distance(y_pred: torch.Tensor,\n",
    "                                   y_true: torch.Tensor) -> torch.Tensor:\n",
    "        if (y_pred.shape[-1] != 2) or (y_true.shape[-1] != 2):\n",
    "            assert RuntimeError('Input tensors require a dimension of two.')\n",
    "\n",
    "        sine_term = torch.sin(y_pred[:, 0]) * torch.sin(y_true[:, 0])\n",
    "        cosine_term = torch.cos(y_pred[:, 0]) * torch.cos(y_true[:, 0]) * torch.cos(y_true[:, 1] - y_pred[:, 1])\n",
    "\n",
    "        return torch.acos(F.hardtanh(sine_term + cosine_term, min_val=-1, max_val=1))\n",
    "    \n",
    "batch = 2\n",
    "num_hyps = 10\n",
    "gts = torch.randn(batch,num_hyps,2)\n",
    "hyps_stacked_t=torch.randn(batch,num_hyps,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0022678375244140625\n"
     ]
    }
   ],
   "source": [
    "time_before = time.time()\n",
    "hyps_stacked_t = hyps_stacked_t.view(-1,2)\n",
    "gts = gts.view(-1,2)\n",
    "diff = compute_spherical_distance(hyps_stacked_t,gts)\n",
    "diff = diff.view(batch,num_hyps)\n",
    "spatial_epes = diff.unsqueeze(2).unsqueeze(3).unsqueeze(4)\n",
    "print(time.time()-time_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0016078948974609375\n"
     ]
    }
   ],
   "source": [
    "# V2\n",
    "# hyps_stacked_t of shape (batch,num_hyps,2), gts of shape (batch, num_hyps, 2)\n",
    "# Compute the diff tensor using tensor operations\n",
    "time_before = time.time()\n",
    "sine_term = torch.sin(hyps_stacked_t[:, :, 0]) * torch.sin(gts[:, :, 0])  # (batch, num_hyps)\n",
    "cosine_term = torch.cos(hyps_stacked_t[:, :, 0]) * torch.cos(gts[:, :, 0]) * torch.cos(gts[:, :, 1] - hyps_stacked_t[:, :, 1])  # (batch, num_hyps)\n",
    "diff2 = torch.acos(torch.clamp(sine_term + cosine_term, min=-1, max=1))  # (batch, num_hyps)\n",
    "spatial_epes2 = diff2.unsqueeze(2).unsqueeze(3).unsqueeze(4)  # (batch, num_hyps, 1, 1, 1) \n",
    "print(time.time()-time_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[True]]],\n",
       "\n",
       "\n",
       "         [[[True]]],\n",
       "\n",
       "\n",
       "         [[[True]]],\n",
       "\n",
       "\n",
       "         [[[True]]],\n",
       "\n",
       "\n",
       "         [[[True]]],\n",
       "\n",
       "\n",
       "         [[[True]]],\n",
       "\n",
       "\n",
       "         [[[True]]],\n",
       "\n",
       "\n",
       "         [[[True]]],\n",
       "\n",
       "\n",
       "         [[[True]]],\n",
       "\n",
       "\n",
       "         [[[True]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[True]]],\n",
       "\n",
       "\n",
       "         [[[True]]],\n",
       "\n",
       "\n",
       "         [[[True]]],\n",
       "\n",
       "\n",
       "         [[[True]]],\n",
       "\n",
       "\n",
       "         [[[True]]],\n",
       "\n",
       "\n",
       "         [[[True]]],\n",
       "\n",
       "\n",
       "         [[[True]]],\n",
       "\n",
       "\n",
       "         [[[True]]],\n",
       "\n",
       "\n",
       "         [[[True]]],\n",
       "\n",
       "\n",
       "         [[[True]]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spatial_epes == spatial_epes2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ansim'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = '/root/workspace/lightning-hydra-template/data/ansim'\n",
    "dataset_path.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# define the input tensor\n",
    "batch = 3\n",
    "max_sources = 4\n",
    "num_hyp = 5\n",
    "T = torch.randn(batch, max_sources, 2)\n",
    "\n",
    "T2 = T.unsqueeze(-1)\n",
    "\n",
    "# # duplicate the components along the fourth dimension\n",
    "# T2 = T.unsqueeze(2).repeat(1, 1, num_hyp, 1)\n",
    "\n",
    "# # check the shapes\n",
    "# print(T.shape)   # should be [batch, max_sources, 2]\n",
    "# print(T2.shape)  # should be [batch, max_sources, num_hyp, 2]\n",
    "\n",
    "# # test the indexing condition\n",
    "# i = 1\n",
    "# j = 2\n",
    "# print(torch.all(T[:, i, :] == T2[:, i, j, :]))  # should be True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 2])\n",
      "torch.Size([3, 4, 5, 2])\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# define the input tensor\n",
    "batch = 3\n",
    "max_sources = 4\n",
    "num_hyp = 5\n",
    "T = torch.randn(batch, max_sources, 2)\n",
    "\n",
    "# duplicate the components along the fourth dimension\n",
    "T2 = T.unsqueeze(2).repeat(1, 1, num_hyp, 1)\n",
    "\n",
    "# check the shapes\n",
    "print(T.shape)   # should be [batch, max_sources, 2]\n",
    "print(T2.shape)  # should be [batch, max_sources, num_hyp, 2]\n",
    "\n",
    "# test the indexing condition\n",
    "i = 1\n",
    "j = 2\n",
    "print(torch.all(T[:, i, :] == T2[:, i, j, :]))  # should be True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draft_make_sampling_loss_ambiguous_gts(hyps_stacked_t, source_activity_target_t, direction_of_arrival_target_t, mode='epe'):\n",
    "        # hyps_stacked_t of shape [batch,self.num_hypothesis,2]\n",
    "        # source_activity_target_t of shape [batch,Max_sources]\n",
    "        # direction_of_arrival_target_t of shape [batch,Max_sources,2]\n",
    "        \n",
    "        filling_value = 10000 #Large number (on purpose) ; computational trick to not considers the \"fake\" ground truths.\n",
    "        # whenever the sources are not active, as the source_activity is not to be deduced by the model is these settings. \n",
    "        num_hyps = hyps_stacked_t.shape[1]\n",
    "        batch = source_activity_target_t.shape[0]\n",
    "        Max_sources = source_activity_target_t.shape[1]\n",
    "        \n",
    "        #1st padding related to the inactive sources, not considered in the error calculation (with high error values)\n",
    "        mask_active_sources = source_activity_target_t == 0\n",
    "        mask_active_sources = mask_active_sources.unsqueeze(-1).expand_as(direction_of_arrival_target_t)\n",
    "        direction_of_arrival_target_t[mask_active_sources] = filling_value #Shape [batch,Max_sources,2]\n",
    "        \n",
    "        #The ground truth tensor created is of shape [batch,Max_sources,num_hypothesis,2], such that each of the \n",
    "        # tensors gts[batch,i,num_hypothesis,2] contains duplicates of direction_of_arrival_target_t along the num_hypothesis\n",
    "        # dimension. Note that for some values of i, gts[batch,i,num_hypothesis,2] may contain inactive sources, and therefore \n",
    "        # gts[batch,i,j,2] will be filled with filling_value (defined above) for each j in the hypothesis dimension.\n",
    "        gts =  direction_of_arrival_target_t.unsqueeze(2).repeat(1,1,num_hyps,1) #Shape [batch,Max_sources,num_hypothesis,2]\n",
    "        \n",
    "        #We duplicate the hyps_stacked with a new dimension of shape Max_sources\n",
    "        hyps_stacked_t_duplicated = hyps_stacked_t.unsqueeze(1).repeat(1,Max_sources,1,1) #Shape [batch,Max_sources,num_hypothesis,2]\n",
    "\n",
    "        epsilon = 0.05\n",
    "        eps = 0.001\n",
    "        \n",
    "        #### With euclidean distance\n",
    "        diff = torch.square(hyps_stacked_t_duplicated-gts) #Shape [batch,Max_sources,num_hypothesis,2]\n",
    "        channels_sum = torch.sum(diff, dim=2) #Sum over the two dimensions (azimuth and elevation here). Shape [batch,Max_sources,num_hypothesis]\n",
    "        spatial_epes = torch.sqrt(channels_sum + eps)  #Distance matrix [batch,Max_sources,num_hypothesis]\n",
    "        \n",
    "        sum_losses = torch.tensor(0.0)\n",
    "\n",
    "        if mode == 'epe': \n",
    "            \n",
    "            spatial_epe, idx_selected = torch.min(spatial_epes, dim=2) #spatial_epe of shape [batch,Max_sources]\n",
    "            mask = spatial_epe >= filling_value/2 #We create a mask for only selecting the actives sources, i.e. those which were not filled with\n",
    "            # direction_of_arrival_target_t[:, source_activity_target_t == 0, :] = filling_value above. \n",
    "            spatial_epe = spatial_epe*mask #[batch,Max_sources], we select only the active sources. \n",
    "            count_non_zero = torch.sum(mask!=0) #We count the number of actives sources for the computation of the mean (below). \n",
    "            loss = torch.sum(spatial_epe)/count_non_zero #We compute the mean of the diff. \n",
    "            sum_losses = torch.add(loss, sum_losses)  \n",
    "            \n",
    "        elif mode == 'epe-relaxed':\n",
    "        \n",
    "            #We compute the loss for the \"best\" hypothesis. \n",
    "            \n",
    "            spatial_epe, idx_selected = torch.min(spatial_epes, dim=2) #spatial_epe of shape [batch,Max_sources], idx_selected of shape [batch].\n",
    "            mask = spatial_epe >= filling_value/2 #We create a mask for only selecting the actives sources, i.e. those which were not filled with\n",
    "            # direction_of_arrival_target_t[:, source_activity_target_t == 0, :] = filling_value above. \n",
    "            spatial_epe = spatial_epe*mask #Shape [batch,Max_sources] ; we select only the active sources. \n",
    "            count_non_zero = torch.sum(mask!=0) #We count the number of actives sources as a sum over the batch for the computation of the mean (below).\n",
    "            loss0 = torch.multiply(torch.sum(spatial_epe)/count_non_zero, 1 - epsilon) #Scalar (average with coefficient)\n",
    "\n",
    "            #We then the find the other hypothesis, and compute the epsilon weighted loss for them\n",
    "            \n",
    "            # At first, we remove hypothesis corresponding to \"fake\" ground-truth.         \n",
    "            large_mask = spatial_epes >= filling_value/2 # We remove entries corresponding to \"fake\"/filled ground truth in the tensor spatial_epes on\n",
    "            # which the min operator was not already applied. Shape [batch,Max_sources,num_hypothesis]\n",
    "            spatial_epes = spatial_epes*large_mask # Shape [batch,Max_sources,num_hypothesis].\n",
    "            \n",
    "            # We then remove the hypothesis selected above (with minimum dist)\n",
    "            mask_selected = torch.zeros_like(spatial_epes) #Shape [batch,Max_sources,num_hypothesis]\n",
    "            mask_selected.scatter_(2, idx_selected.unsqueeze(-1), 1) #Shape [batch,Max_sources,num_hypothesis]\n",
    "            mask_selected = ~mask_selected #Shape [batch,Max_sources,num_hypothesis], we keep only the hypothesis which are not the minimum.\n",
    "            spatial_epes = spatial_epes * mask_selected #Shape [batch,Max_sources,num_hypothesis]\n",
    "            \n",
    "            # mask_active_sources = mask_active_sources.unsqueeze(-1).expand_as(direction_of_arrival_target_t)\n",
    "            # direction_of_arrival_target_t[mask_active_sources] = filling_value #Shape [batch,Max_sources,2]\n",
    "            \n",
    "            # Finally, we compute the loss\n",
    "            count_non_zeros = torch.sum(spatial_epes!=0)\n",
    "            if count_non_zeros > 0 :\n",
    "                loss = torch.multiply(torch.sum(spatial_epes)/count_non_zero, epsilon) #Scalar for each hyp\n",
    "            \n",
    "            sum_losses = torch.add(loss, sum_losses)\n",
    "            sum_losses = torch.add(loss0, sum_losses)\n",
    "\n",
    "        return sum_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 5\n",
    "Max_sources = 3\n",
    "num_hypothesis = 2\n",
    "\n",
    "spatial_epes = torch.randn(batch,Max_sources,num_hypothesis)\n",
    "spatial_epe, idx_selected = torch.min(spatial_epes, dim=2) \n",
    "\n",
    "# print(idx_selected.shape)\n",
    "mask_selected = torch.zeros_like(spatial_epes,dtype=bool)\n",
    "mask_selected.scatter_(2, idx_selected.unsqueeze(-1), 1)\n",
    "mask_selected = ~mask_selected\n",
    "\n",
    "spatial_epes = spatial_epes * mask_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False],\n",
       "         [ True, False],\n",
       "         [False,  True]],\n",
       "\n",
       "        [[False,  True],\n",
       "         [False,  True],\n",
       "         [False,  True]],\n",
       "\n",
       "        [[False,  True],\n",
       "         [ True, False],\n",
       "         [ True, False]],\n",
       "\n",
       "        [[False,  True],\n",
       "         [ True, False],\n",
       "         [False,  True]],\n",
       "\n",
       "        [[ True, False],\n",
       "         [False,  True],\n",
       "         [False,  True]]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "~mask_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4117, -0.6108])\n",
      "tensor([1., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(spatial_epes[0,0,:])\n",
    "print(mask_selected[0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch = 5\n",
    "Max_sources = 3\n",
    "num_hypothesis = 2\n",
    "\n",
    "# create the input tensor\n",
    "spatial_epes = torch.randn(batch, Max_sources, num_hypothesis)\n",
    "\n",
    "# Original implementation\n",
    "spatial_epe_orig, idx_selected_orig = torch.min(spatial_epes, dim=2) \n",
    "mask_selected_orig = torch.zeros_like(spatial_epes)\n",
    "\n",
    "for b in range(batch) :\n",
    "    for s in range(Max_sources) :\n",
    "        mask_selected_orig[b,s,idx_selected_orig[b,s]]=1\n",
    "\n",
    "mask_selected_orig = 1 - mask_selected_orig\n",
    "\n",
    "# Optimized implementation\n",
    "spatial_epe_opt, idx_selected_opt = torch.min(spatial_epes, dim=2)\n",
    "mask_selected_opt = torch.zeros_like(spatial_epes)\n",
    "mask_selected_opt.scatter_(2, idx_selected_opt.unsqueeze(-1), 1)\n",
    "mask_selected_opt = 1 - mask_selected_opt\n",
    "\n",
    "# Check if the two implementations are equivalent\n",
    "assert torch.allclose(spatial_epe_orig, spatial_epe_opt)\n",
    "assert torch.allclose(idx_selected_orig, idx_selected_opt)\n",
    "assert torch.allclose(mask_selected_orig, mask_selected_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7, 6, 4],\n",
       "        [3, 1, 1],\n",
       "        [4, 4, 7],\n",
       "        [7, 9, 9],\n",
       "        [5, 1, 7]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "scatter_() received an invalid combination of arguments - got (src=int, dim=int, index=Tensor, ), but expected one of:\n * (int dim, Tensor index, Tensor src)\n      didn't match because some of the arguments have invalid types: (dim=int, index=Tensor, !src=int!, )\n * (int dim, Tensor index, Tensor src, *, str reduce)\n * (int dim, Tensor index, Number value)\n      didn't match because some of the keywords were incorrect: src\n * (int dim, Tensor index, Number value, *, str reduce)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35065/2342142445.py\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# use scatter_() to set the values at the indices of the minimum values to 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: scatter_() received an invalid combination of arguments - got (src=int, dim=int, index=Tensor, ), but expected one of:\n * (int dim, Tensor index, Tensor src)\n      didn't match because some of the arguments have invalid types: (dim=int, index=Tensor, !src=int!, )\n * (int dim, Tensor index, Tensor src, *, str reduce)\n * (int dim, Tensor index, Number value)\n      didn't match because some of the keywords were incorrect: src\n * (int dim, Tensor index, Number value, *, str reduce)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# create a tensor x\n",
    "x = torch.tensor([[1, 2, 3, 4], [5, 1, 7, 8], [9, 10, 1, 12]])\n",
    "\n",
    "# find the indices of the minimum values along the second dimension\n",
    "min_indices = torch.argmin(x, dim=1)\n",
    "\n",
    "# create a mask of zeros with the same shape as x\n",
    "mask = torch.zeros_like(x)\n",
    "\n",
    "# use scatter_() to set the values at the indices of the minimum values to 0\n",
    "mask.scatter_(1, min_indices.unsqueeze(1), 1)\n",
    "x = x * (1 - mask)\n",
    "\n",
    "# print the results\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 2, 7],\n",
       "        [8, 0, 7],\n",
       "        [4, 2, 8],\n",
       "        [0, 0, 0],\n",
       "        [7, 8, 8]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 5],\n",
       "        [8, 4, 8],\n",
       "        [8, 1, 8],\n",
       "        [6, 1, 6],\n",
       "        [6, 0, 0]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[17320.4922,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000, 17320.6484],\n",
      "        [    0.0000, 17319.1191, 17319.1191],\n",
      "        [17319.9727, 17319.9727,     0.0000]])\n",
      "torch.Size([5, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected index [5, 1, 3] to be smaller than self [5, 3, 2] apart from dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35065/3027060923.py\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msource_activity_target_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMax_sources\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdirection_of_arrival_target_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMax_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdraft_make_sampling_loss_ambiguous_gts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyps_stacked_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_activity_target_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection_of_arrival_target_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'epe-relaxed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_35065/1796078851.py\u001b[0m in \u001b[0;36mdraft_make_sampling_loss_ambiguous_gts\u001b[0;34m(hyps_stacked_t, source_activity_target_t, direction_of_arrival_target_t, mode)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;31m# We then remove the hypothesis selected above (with minimum dist)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mmask_selected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspatial_epes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Shape [batch,Max_sources,num_hypothesis].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mmask_selected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_selected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#We put 0 on the selected hypothesis and 1 elsewhere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mspatial_epes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspatial_epes\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask_selected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected index [5, 1, 3] to be smaller than self [5, 3, 2] apart from dimension 1"
     ]
    }
   ],
   "source": [
    "### Tests of the draft_make_sampling_loss_ambiguous_gts function\n",
    "\n",
    "batch = 5\n",
    "Max_sources = 3\n",
    "num_hypothesis = 10\n",
    "hyps_stacked_t = torch.randn(batch,Max_sources,2)\n",
    "source_activity_target_t = torch.randint(high=2, size=(batch,Max_sources))\n",
    "direction_of_arrival_target_t = torch.randn(batch,Max_sources,2)\n",
    "draft_make_sampling_loss_ambiguous_gts(hyps_stacked_t, source_activity_target_t, direction_of_arrival_target_t, 'epe-relaxed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filling_value = torch.tensor([10000,10000])\n",
    "\n",
    "for b in range(batch) :\n",
    "    for s in range(max_sources) : \n",
    "        if source_activity_target_t[b,s] == 0 :\n",
    "            direction_of_arrival_target_t[b,s,:]=filling_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False,  True],\n",
      "        [ True, False,  True],\n",
      "        [ True,  True, False],\n",
      "        [False,  True, False],\n",
      "        [ True, False,  True]])\n",
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3, 2])\n",
      "torch.Size([5, 3, 2])\n",
      "tensor([[[-1.4688e+00,  8.8400e-01],\n",
      "         [-7.7471e-01, -2.3873e+00],\n",
      "         [ 1.0000e+03,  1.0000e+03]],\n",
      "\n",
      "        [[ 1.0000e+03,  1.0000e+03],\n",
      "         [-7.3306e-02,  3.8826e-01],\n",
      "         [ 1.0000e+03,  1.0000e+03]],\n",
      "\n",
      "        [[ 1.0000e+03,  1.0000e+03],\n",
      "         [ 1.0000e+03,  1.0000e+03],\n",
      "         [ 3.1762e-01,  4.1972e-01]],\n",
      "\n",
      "        [[ 7.7035e-01,  9.5233e-02],\n",
      "         [ 1.0000e+03,  1.0000e+03],\n",
      "         [ 4.3323e-01, -4.4909e-01]],\n",
      "\n",
      "        [[ 1.0000e+03,  1.0000e+03],\n",
      "         [ 4.9183e-01,  1.7220e-01],\n",
      "         [ 1.0000e+03,  1.0000e+03]]])\n"
     ]
    }
   ],
   "source": [
    "mask_active_sources = source_activity_target_t == 0\n",
    "print(mask_active_sources)\n",
    "print(mask_active_sources.shape)\n",
    "mask_active_sources = mask_active_sources.unsqueeze(-1).expand_as(direction_of_arrival_target_t)\n",
    "print(mask_active_sources.shape)\n",
    "print(direction_of_arrival_target_t.shape)\n",
    "direction_of_arrival_target_t[mask_active_sources]=1000\n",
    "print(direction_of_arrival_target_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_active_sources.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Index put requires the source and destination dtypes match, got Float for the destination and Long for the source.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35065/3459672377.py\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource_activity_target_t\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mdirection_of_arrival_target_t_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirection_of_arrival_target_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mdirection_of_arrival_target_t_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilling_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# check if the results are the same\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Index put requires the source and destination dtypes match, got Float for the destination and Long for the source."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set the batch size and maximum number of sources\n",
    "batch_size = 3\n",
    "max_sources = 4\n",
    "\n",
    "# create random tensors for source_activity_target_t and direction_of_arrival_target_t\n",
    "source_activity_target_t = torch.randint(high=2, size=(batch_size, max_sources))\n",
    "direction_of_arrival_target_t = torch.randn(batch_size, max_sources, 2)\n",
    "\n",
    "# create the filling_value tensor\n",
    "filling_value = torch.tensor([10000, 10000])\n",
    "\n",
    "# apply the original code\n",
    "for b in range(batch_size):\n",
    "    for s in range(max_sources): \n",
    "        if source_activity_target_t[b, s] == 0:\n",
    "            direction_of_arrival_target_t[b, s, :] = filling_value\n",
    "\n",
    "# apply the new code\n",
    "mask = source_activity_target_t == 0\n",
    "direction_of_arrival_target_t_new = direction_of_arrival_target_t.clone()\n",
    "direction_of_arrival_target_t_new[mask] = filling_value\n",
    "\n",
    "# check if the results are the same\n",
    "assert torch.allclose(direction_of_arrival_target_t, direction_of_arrival_target_t_new, rtol=1e-5, atol=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False,  True],\n",
       "        [ True,  True, False],\n",
       "        [ True, False,  True],\n",
       "        [ True, False, False],\n",
       "        [ True, False,  True]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_activity_target_t==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
